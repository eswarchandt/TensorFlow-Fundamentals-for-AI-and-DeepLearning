{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqGUr5dmWO3w"
   },
   "outputs": [],
   "source": [
    "#!pip install -U tensorflow==2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTGGLFgEXbab"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jrXNTqSbXJPx",
    "outputId": "cccfa358-a7c2-4ee8-a8cc-cd5052097384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(54.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "A, B = tf.constant( 3.0), tf.constant( 6.0) \n",
    "X = tf.Variable( 20.0) # In practice, we would start with a random value \n",
    "loss = tf.math.abs( A * X - B)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Syr2UuSPXqiD"
   },
   "source": [
    "Tensorflow executedthe loss function and emitted the result in the print command. With no way to store the result, there is no way to do graident descent. This is where the Gradient Tape plays an important role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "ca2ZQifpXxs6",
    "outputId": "5af56169-0d17-4947-c12e-71e3e4fe90e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X =  20.0\n",
      "dx =  3.0\n",
      " X =  17.0\n",
      "dx =  3.0\n",
      " X =  14.0\n",
      "dx =  3.0\n",
      " X =  11.0\n",
      "dx =  3.0\n",
      " X =  8.0\n",
      "dx =  3.0\n",
      " X =  5.0\n",
      "dx =  3.0\n",
      " X =  2.0\n",
      "dx =  0.0\n",
      " X =  2.0\n",
      "dx =  0.0\n",
      " X =  2.0\n",
      "dx =  0.0\n",
      " X =  2.0\n",
      "dx =  0.0\n"
     ]
    }
   ],
   "source": [
    "def train_step(): \n",
    "  with tf.GradientTape() as tape: \n",
    "    loss = tf.math.abs( A * X - B) \n",
    "    dX = tape.gradient( loss, X) \n",
    "    print(' X = ', X.numpy())\n",
    "    print('dx = ',  dX.numpy()) \n",
    "    X.assign( X - dX) \n",
    "    \n",
    "for i in range( 10): \n",
    "  train_step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyRwKnDvZsfK"
   },
   "source": [
    "The Gradient Tape acts as a tape recorder to record all the calculations including the gradient which is used to update the X until running out of loop counter. It also provides a gradient function. We don't need to write it"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "GradientTape.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
